{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section Project: Federated Learning with Encrypted Gradient Aggregation\n",
    "\n",
    "For the final project for this section, you're going to perform federated learning using the encryption and secret sharing methods you learned in the section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T14:12:24.140497Z",
     "start_time": "2019-07-07T14:12:19.845313Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tf_encrypted:Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow (1.13.1). Fix this by compiling custom ops.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.distributions as dists\n",
    "import torch.utils.data as data\n",
    "\n",
    "from fixed_adam import Adam\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import syft\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "hook = syft.TorchHook(torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T14:12:24.151466Z",
     "start_time": "2019-07-07T14:12:24.145450Z"
    }
   },
   "outputs": [],
   "source": [
    "n_workers = 30\n",
    "\n",
    "workers = [syft.VirtualWorker(hook, id=\"Worker:{:d}\".format(i)) for i in range(n_workers)]\n",
    "\n",
    "# for i in range(len(workers)):\n",
    "#     workers[i].add_workers(workers[:i] + workers[i+1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the MNIST Training & Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T14:12:24.199306Z",
     "start_time": "2019-07-07T14:12:24.155422Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Size: 60000\n",
      "Test Set Size: 10000\n"
     ]
    }
   ],
   "source": [
    "mnist_trainset = datasets.MNIST(root='../data', train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_testset  = datasets.MNIST(root='../data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "print(\"Training Set Size:\", len(mnist_trainset))\n",
    "print(\"Test Set Size:\", len(mnist_testset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Federated Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T14:12:33.765514Z",
     "start_time": "2019-07-07T14:12:24.201300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FederatedDataset\n",
      "    Distributed accross: Worker:0, Worker:1, Worker:2, Worker:3, Worker:4, Worker:5, Worker:6, Worker:7, Worker:8, Worker:9, Worker:10, Worker:11, Worker:12, Worker:13, Worker:14, Worker:15, Worker:16, Worker:17, Worker:18, Worker:19, Worker:20, Worker:21, Worker:22, Worker:23, Worker:24, Worker:25, Worker:26, Worker:27, Worker:28, Worker:29\n",
      "    Number of datapoints: 60000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "federated_mnist_trainset = mnist_trainset.federate(workers)\n",
    "print(federated_mnist_trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T14:12:33.776434Z",
     "start_time": "2019-07-07T14:12:33.767511Z"
    }
   },
   "outputs": [],
   "source": [
    "class MNISTClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTClassifier, self).__init__()\n",
    "\n",
    "        # 1x28x28\n",
    "        self.conv0      = nn.Conv2d(1, 4, 3, padding=1)\n",
    "        self.maxpool0   = nn.MaxPool2d(2)\n",
    "        # 4x14x14\n",
    "        self.conv1      = nn.Conv2d(4, 6, 3, padding=1)\n",
    "        self.maxpool1   = nn.MaxPool2d(2)\n",
    "        # 6x 7x 7\n",
    "        self.conv2      = nn.Conv2d(6, 8, 3, padding=1)\n",
    "        self.maxpool2   = nn.MaxPool2d(2, padding=1)\n",
    "        # 8x 4x 4 = 128\n",
    "        self.fc         = nn.Linear(128, 10)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv0(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.maxpool0(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.fc(x.view(-1, 128))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encrypted Federated Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Averaged Model Parameters Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T14:12:33.794438Z",
     "start_time": "2019-07-07T14:12:33.778937Z"
    }
   },
   "outputs": [],
   "source": [
    "model = MNISTClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T14:23:32.182287Z",
     "start_time": "2019-07-07T14:12:33.797430Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "worker 29/29 - Step 9/9 | Loss=2.2068 | Accuracy=0.1719        \n",
      "    Test Loss: 2.2098304954528807\n",
      "    Test Accuracy: 0.3036\n",
      "Epoch 1:\n",
      "worker 29/29 - Step 9/9 | Loss=1.4151 | Accuracy=0.5469        \n",
      "    Test Loss: 1.2848269987106322\n",
      "    Test Accuracy: 0.6728\n",
      "Epoch 2:\n",
      "worker 29/29 - Step 9/9 | Loss=0.5334 | Accuracy=0.8125        \n",
      "    Test Loss: 0.6508465280532837\n",
      "    Test Accuracy: 0.7922\n",
      "Epoch 3:\n",
      "worker 29/29 - Step 9/9 | Loss=0.4554 | Accuracy=0.8594        \n",
      "    Test Loss: 0.46136472125053407\n",
      "    Test Accuracy: 0.8697\n",
      "Epoch 4:\n",
      "worker 29/29 - Step 9/9 | Loss=0.4278 | Accuracy=0.8750        \n",
      "    Test Loss: 0.3562229021072388\n",
      "    Test Accuracy: 0.8954\n",
      "Epoch 5:\n",
      "worker 29/29 - Step 9/9 | Loss=0.1859 | Accuracy=0.9531        \n",
      "    Test Loss: 0.2978161085605621\n",
      "    Test Accuracy: 0.9113\n",
      "Epoch 6:\n",
      "worker 29/29 - Step 9/9 | Loss=0.3252 | Accuracy=0.9219        \n",
      "    Test Loss: 0.2570676740407944\n",
      "    Test Accuracy: 0.9216\n",
      "Epoch 7:\n",
      "worker 29/29 - Step 9/9 | Loss=0.2954 | Accuracy=0.9062        \n",
      "    Test Loss: 0.22425860333442688\n",
      "    Test Accuracy: 0.931\n",
      "Epoch 8:\n",
      "worker 29/29 - Step 9/9 | Loss=0.1457 | Accuracy=0.9688        \n",
      "    Test Loss: 0.20628514063358308\n",
      "    Test Accuracy: 0.936\n",
      "Epoch 9:\n",
      "worker 29/29 - Step 9/9 | Loss=0.1660 | Accuracy=0.9375        \n",
      "    Test Loss: 0.18552312121391296\n",
      "    Test Accuracy: 0.9444\n"
     ]
    }
   ],
   "source": [
    "n_epochs   = 10\n",
    "n_steps    = 10\n",
    "lr         = 1e-2\n",
    "batch_size = 64\n",
    "\n",
    "local_models     = [model.copy().send(worker) for worker in workers]\n",
    "local_optimizers = [Adam(local_model.parameters(), lr=lr) for local_model in local_models]\n",
    "criterion        = nn.CrossEntropyLoss()\n",
    "test_dataloader  = data.DataLoader(mnist_testset, batch_size=1024)\n",
    "\n",
    "for i_epoch in range(n_epochs):\n",
    "    print(\"Epoch {:d}:\".format(i_epoch))\n",
    "\n",
    "    print(\"worker 0/{:d} - Step 0/{:d}                                \".format(len(workers)-1, n_steps-1), end='\\r')\n",
    "    for i, (worker, local_model, local_optimizer) in enumerate(zip(workers, local_models, local_optimizers)):\n",
    "\n",
    "        dataset = federated_mnist_trainset.datasets[worker.id]\n",
    "        dataloader = data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        for i_step in range(n_steps):\n",
    "\n",
    "            imgs, labels = next(iter(dataloader))\n",
    "\n",
    "            preds = local_model(imgs)\n",
    "\n",
    "            local_optimizer.zero_grad()\n",
    "            loss = criterion(preds, labels)\n",
    "            loss.backward()\n",
    "            local_optimizer.step()\n",
    "\n",
    "            loss = loss.data.clone().get().item()\n",
    "            acc  = (preds.argmax(dim=1) == labels).float().mean().get().item()\n",
    "            print(\"worker {:d}/{:d} - Step {:d}/{:d} | Loss={:.4f} | Accuracy={:.4f}        \".format(i, len(workers)-1, i_step, n_steps-1, loss, acc), end='\\r')\n",
    "\n",
    "    ### [Clarification Requested]\n",
    "    ### Loop through the main model parameters and corresponding parameters of the independently trained local models\n",
    "    for global_param, local_params in zip(model.parameters(), zip(*[local_model.parameters() for local_model in local_models])):\n",
    "        ### Share each local parameter to all other workers\n",
    "        ### (I'm not sure whether having each of the cryptoprovider as the original owner is a right way to do this...)\n",
    "        ### (Maybe I should use a separate trusted crypto provider?)\n",
    "        local_param_shares = [local_param.clone().fix_prec().share(*workers, crypto_provider=worker).get()\n",
    "                              for local_param, worker in zip(local_params, workers)]\n",
    "        ### Sum the shares, retrieve them, turn the fixed precision values into floating ones,\n",
    "        ### and get the average by dividing them by the number of local parameters\n",
    "        avg_param = sum(local_param_shares).get().float_prec() / len(local_params)\n",
    "        ### Update the main model's parameter with the average parameter of the local models\n",
    "        global_param.data.copy_(avg_param)\n",
    "        ### Update the local parameters by sending a copy of the updated main parameter to each worker\n",
    "        for local_param, worker in zip(local_params, workers):\n",
    "            local_param.data.copy_(global_param.data.clone().send(worker))\n",
    "\n",
    "    test_loss      = 0\n",
    "    instance_count = 0\n",
    "    correct_count  = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in test_dataloader:\n",
    "            instance_count += imgs.size(0)\n",
    "\n",
    "            preds = model(imgs)\n",
    "\n",
    "            test_loss += criterion(preds, labels).item() * imgs.size(0)\n",
    "            correct_count += (preds.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "    print()\n",
    "    print(\"    Test Loss:\", test_loss / instance_count)\n",
    "    print(\"    Test Accuracy:\", correct_count / instance_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Averaged Gradients Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T14:23:32.191786Z",
     "start_time": "2019-07-07T14:23:32.184283Z"
    }
   },
   "outputs": [],
   "source": [
    "model = MNISTClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T14:50:59.430158Z",
     "start_time": "2019-07-07T14:23:32.195287Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "worker 29/29\n",
      "    Training Loss: 2.3048917134602864\n",
      "    Training Accuracy: 0.084375\n",
      "Epoch 1:\n",
      "worker 29/29\n",
      "    Training Loss: 2.2998311360677084\n",
      "    Training Accuracy: 0.11354166666666667\n",
      "Epoch 2:\n",
      "worker 29/29\n",
      "    Training Loss: 2.2929842631022135\n",
      "    Training Accuracy: 0.11666666666666667\n",
      "Epoch 3:\n",
      "worker 29/29\n",
      "    Training Loss: 2.2694302876790364\n",
      "    Training Accuracy: 0.11354166666666667\n",
      "Epoch 4:\n",
      "worker 29/29\n",
      "    Training Loss: 2.2462562561035155\n",
      "    Training Accuracy: 0.09895833333333333\n",
      "Epoch 5:\n",
      "worker 29/29\n",
      "    Training Loss: 2.1893885294596354\n",
      "    Training Accuracy: 0.221875\n",
      "Epoch 6:\n",
      "worker 29/29\n",
      "    Training Loss: 2.0800530751546225\n",
      "    Training Accuracy: 0.37395833333333334\n",
      "Epoch 7:\n",
      "worker 29/29\n",
      "    Training Loss: 1.925430170694987\n",
      "    Training Accuracy: 0.453125\n",
      "Epoch 8:\n",
      "worker 29/29\n",
      "    Training Loss: 1.72519900004069\n",
      "    Training Accuracy: 0.5791666666666667\n",
      "Epoch 9:\n",
      "worker 29/29\n",
      "    Training Loss: 1.5279937744140626\n",
      "    Training Accuracy: 0.5458333333333333\n",
      "Epoch 10:\n",
      "worker 29/29\n",
      "    Training Loss: 1.2718708038330078\n",
      "    Training Accuracy: 0.6145833333333334\n",
      "Epoch 11:\n",
      "worker 29/29\n",
      "    Training Loss: 1.1764864603678384\n",
      "    Training Accuracy: 0.6052083333333333\n",
      "Epoch 12:\n",
      "worker 29/29\n",
      "    Training Loss: 1.188311513264974\n",
      "    Training Accuracy: 0.609375\n",
      "Epoch 13:\n",
      "worker 29/29\n",
      "    Training Loss: 0.9863187789916992\n",
      "    Training Accuracy: 0.6760416666666667\n",
      "Epoch 14:\n",
      "worker 29/29\n",
      "    Training Loss: 1.0507333119710287\n",
      "    Training Accuracy: 0.6666666666666666\n",
      "Epoch 15:\n",
      "worker 29/29\n",
      "    Training Loss: 1.0161010106404622\n",
      "    Training Accuracy: 0.6770833333333334\n",
      "Epoch 16:\n",
      "worker 29/29\n",
      "    Training Loss: 0.7645322799682617\n",
      "    Training Accuracy: 0.7416666666666667\n",
      "Epoch 17:\n",
      "worker 29/29\n",
      "    Training Loss: 0.7615791956583658\n",
      "    Training Accuracy: 0.740625\n",
      "Epoch 18:\n",
      "worker 29/29\n",
      "    Training Loss: 0.8026187260945638\n",
      "    Training Accuracy: 0.728125\n",
      "Epoch 19:\n",
      "worker 29/29\n",
      "    Training Loss: 0.6545291900634765\n",
      "    Training Accuracy: 0.8020833333333334\n",
      "Epoch 20:\n",
      "worker 29/29\n",
      "    Training Loss: 0.5589344024658203\n",
      "    Training Accuracy: 0.8291666666666667\n",
      "Epoch 21:\n",
      "worker 29/29\n",
      "    Training Loss: 0.6135604222615559\n",
      "    Training Accuracy: 0.803125\n",
      "Epoch 22:\n",
      "worker 29/29\n",
      "    Training Loss: 0.5820281346638997\n",
      "    Training Accuracy: 0.8125\n",
      "Epoch 23:\n",
      "worker 29/29\n",
      "    Training Loss: 0.5636541366577148\n",
      "    Training Accuracy: 0.8166666666666667\n",
      "Epoch 24:\n",
      "worker 29/29\n",
      "    Training Loss: 0.5490458170572917\n",
      "    Training Accuracy: 0.8354166666666667\n",
      "Epoch 25:\n",
      "worker 29/29\n",
      "    Training Loss: 0.44374167124430336\n",
      "    Training Accuracy: 0.8614583333333333\n",
      "Epoch 26:\n",
      "worker 29/29\n",
      "    Training Loss: 0.4914083480834961\n",
      "    Training Accuracy: 0.8510416666666667\n",
      "Epoch 27:\n",
      "worker 29/29\n",
      "    Training Loss: 0.40339686075846354\n",
      "    Training Accuracy: 0.871875\n",
      "Epoch 28:\n",
      "worker 29/29\n",
      "    Training Loss: 0.40457293192545574\n",
      "    Training Accuracy: 0.875\n",
      "Epoch 29:\n",
      "worker 29/29\n",
      "    Training Loss: 0.4230250040690104\n",
      "    Training Accuracy: 0.871875\n",
      "Epoch 30:\n",
      "worker 29/29\n",
      "    Training Loss: 0.3875635464986165\n",
      "    Training Accuracy: 0.884375\n",
      "Epoch 31:\n",
      "worker 29/29\n",
      "    Training Loss: 0.38436772028605143\n",
      "    Training Accuracy: 0.8802083333333334\n",
      "Epoch 32:\n",
      "worker 29/29\n",
      "    Training Loss: 0.34722811381022134\n",
      "    Training Accuracy: 0.8989583333333333\n",
      "Epoch 33:\n",
      "worker 29/29\n",
      "    Training Loss: 0.426688543955485\n",
      "    Training Accuracy: 0.8729166666666667\n",
      "Epoch 34:\n",
      "worker 29/29\n",
      "    Training Loss: 0.2586958408355713\n",
      "    Training Accuracy: 0.9177083333333333\n",
      "Epoch 35:\n",
      "worker 29/29\n",
      "    Training Loss: 0.30285936991373696\n",
      "    Training Accuracy: 0.9072916666666667\n",
      "Epoch 36:\n",
      "worker 29/29\n",
      "    Training Loss: 0.3531739552815755\n",
      "    Training Accuracy: 0.9010416666666666\n",
      "Epoch 37:\n",
      "worker 29/29\n",
      "    Training Loss: 0.29793230692545575\n",
      "    Training Accuracy: 0.9020833333333333\n",
      "Epoch 38:\n",
      "worker 29/29\n",
      "    Training Loss: 0.31541353861490884\n",
      "    Training Accuracy: 0.9041666666666667\n",
      "Epoch 39:\n",
      "worker 29/29\n",
      "    Training Loss: 0.24320832888285318\n",
      "    Training Accuracy: 0.921875\n",
      "Epoch 40:\n",
      "worker 29/29\n",
      "    Training Loss: 0.3080958366394043\n",
      "    Training Accuracy: 0.9\n",
      "Epoch 41:\n",
      "worker 29/29\n",
      "    Training Loss: 0.2809510548909505\n",
      "    Training Accuracy: 0.9125\n",
      "Epoch 42:\n",
      "worker 29/29\n",
      "    Training Loss: 0.2848927179972331\n",
      "    Training Accuracy: 0.90625\n",
      "Epoch 43:\n",
      "worker 29/29\n",
      "    Training Loss: 0.29767707188924153\n",
      "    Training Accuracy: 0.9177083333333333\n",
      "Epoch 44:\n",
      "worker 29/29\n",
      "    Training Loss: 0.22290104230244953\n",
      "    Training Accuracy: 0.934375\n",
      "Epoch 45:\n",
      "worker 29/29\n",
      "    Training Loss: 0.2533531188964844\n",
      "    Training Accuracy: 0.9239583333333333\n",
      "Epoch 46:\n",
      "worker 29/29\n",
      "    Training Loss: 0.18897604942321777\n",
      "    Training Accuracy: 0.9427083333333334\n",
      "Epoch 47:\n",
      "worker 29/29\n",
      "    Training Loss: 0.22992812792460124\n",
      "    Training Accuracy: 0.934375\n",
      "Epoch 48:\n",
      "worker 29/29\n",
      "    Training Loss: 0.20525728861490886\n",
      "    Training Accuracy: 0.9354166666666667\n",
      "Epoch 49:\n",
      "worker 29/29\n",
      "    Training Loss: 0.22709062894185383\n",
      "    Training Accuracy: 0.925\n",
      "Epoch 50:\n",
      "worker 29/29\n",
      "    Training Loss: 0.22121354738871257\n",
      "    Training Accuracy: 0.9333333333333333\n",
      "Epoch 51:\n",
      "worker 29/29\n",
      "    Training Loss: 0.2141208330790202\n",
      "    Training Accuracy: 0.934375\n",
      "Epoch 52:\n",
      "worker 29/29\n",
      "    Training Loss: 0.21230312983194988\n",
      "    Training Accuracy: 0.9395833333333333\n",
      "Epoch 53:\n",
      "worker 29/29\n",
      "    Training Loss: 0.22648229598999023\n",
      "    Training Accuracy: 0.934375\n",
      "Epoch 54:\n",
      "worker 29/29\n",
      "    Training Loss: 0.19457290967305502\n",
      "    Training Accuracy: 0.9447916666666667\n",
      "Epoch 55:\n",
      "worker 29/29\n",
      "    Training Loss: 0.21492395401000977\n",
      "    Training Accuracy: 0.9322916666666666\n",
      "Epoch 56:\n",
      "worker 29/29\n",
      "    Training Loss: 0.20023125012715656\n",
      "    Training Accuracy: 0.9416666666666667\n",
      "Epoch 57:\n",
      "worker 29/29\n",
      "    Training Loss: 0.24693438212076824\n",
      "    Training Accuracy: 0.9354166666666667\n",
      "Epoch 58:\n",
      "worker 29/29\n",
      "    Training Loss: 0.21179792086283367\n",
      "    Training Accuracy: 0.9364583333333333\n",
      "Epoch 59:\n",
      "worker 29/29\n",
      "    Training Loss: 0.1825135389963786\n",
      "    Training Accuracy: 0.9520833333333333\n",
      "Epoch 60:\n",
      "worker 29/29\n",
      "    Training Loss: 0.2038697878519694\n",
      "    Training Accuracy: 0.9385416666666667\n",
      "Epoch 61:\n",
      "worker 29/29\n",
      "    Training Loss: 0.16816875139872234\n",
      "    Training Accuracy: 0.9479166666666666\n",
      "Epoch 62:\n",
      "worker 29/29\n",
      "    Training Loss: 0.1624343713124593\n",
      "    Training Accuracy: 0.9479166666666666\n",
      "Epoch 63:\n",
      "worker 29/29\n",
      "    Training Loss: 0.15651354789733887\n",
      "    Training Accuracy: 0.9479166666666666\n",
      "Epoch 64:\n",
      "worker 29/29\n",
      "    Training Loss: 0.2228999932607015\n",
      "    Training Accuracy: 0.9416666666666667\n",
      "Epoch 65:\n",
      "worker 29/29\n",
      "    Training Loss: 0.22328750292460123\n",
      "    Training Accuracy: 0.93125\n",
      "Epoch 66:\n",
      "worker 29/29\n",
      "    Training Loss: 0.19015208880106607\n",
      "    Training Accuracy: 0.9364583333333333\n",
      "Epoch 67:\n",
      "worker 29/29\n",
      "    Training Loss: 0.1569322903951009\n",
      "    Training Accuracy: 0.9552083333333333\n",
      "Epoch 68:\n",
      "worker 29/29\n",
      "    Training Loss: 0.20037708282470704\n",
      "    Training Accuracy: 0.9364583333333333\n",
      "Epoch 69:\n",
      "worker 29/29\n",
      "    Training Loss: 0.16697500546773275\n",
      "    Training Accuracy: 0.9479166666666666\n",
      "Epoch 70:\n",
      "worker 29/29\n",
      "    Training Loss: 0.17373854319254559\n",
      "    Training Accuracy: 0.946875\n",
      "Epoch 71:\n",
      "worker 29/29\n",
      "    Training Loss: 0.16631561915079754\n",
      "    Training Accuracy: 0.9458333333333333\n",
      "Epoch 72:\n",
      "worker 29/29\n",
      "    Training Loss: 0.1801781177520752\n",
      "    Training Accuracy: 0.9541666666666667\n",
      "Epoch 73:\n",
      "worker 29/29\n",
      "    Training Loss: 0.18579999605814615\n",
      "    Training Accuracy: 0.9510416666666667\n",
      "Epoch 74:\n",
      "worker 29/29\n",
      "    Training Loss: 0.1363895893096924\n",
      "    Training Accuracy: 0.9583333333333334\n",
      "Epoch 75:\n",
      "worker 29/29\n",
      "    Training Loss: 0.1488166650136312\n",
      "    Training Accuracy: 0.9541666666666667\n",
      "Epoch 76:\n",
      "worker 29/29\n",
      "    Training Loss: 0.13736457824707032\n",
      "    Training Accuracy: 0.9583333333333334\n",
      "Epoch 77:\n",
      "worker 29/29\n",
      "    Training Loss: 0.1353072961171468\n",
      "    Training Accuracy: 0.9583333333333334\n",
      "Epoch 78:\n",
      "worker 29/29\n",
      "    Training Loss: 0.18216250737508138\n",
      "    Training Accuracy: 0.9458333333333333\n",
      "Epoch 79:\n",
      "worker 29/29\n",
      "    Training Loss: 0.14395313262939452\n",
      "    Training Accuracy: 0.9583333333333334\n",
      "Epoch 80:\n",
      "worker 29/29\n",
      "    Training Loss: 0.14121979077657063\n",
      "    Training Accuracy: 0.9541666666666667\n",
      "Epoch 81:\n",
      "worker 29/29\n",
      "    Training Loss: 0.11884374618530273\n",
      "    Training Accuracy: 0.959375\n",
      "Epoch 82:\n",
      "worker 29/29\n",
      "    Training Loss: 0.140373961130778\n",
      "    Training Accuracy: 0.959375\n",
      "Epoch 83:\n",
      "worker 29/29\n",
      "    Training Loss: 0.15543020566304525\n",
      "    Training Accuracy: 0.953125\n",
      "Epoch 84:\n",
      "worker 29/29\n",
      "    Training Loss: 0.13538020451863605\n",
      "    Training Accuracy: 0.95625\n",
      "Epoch 85:\n",
      "worker 29/29\n",
      "    Training Loss: 0.12838125228881836\n",
      "    Training Accuracy: 0.965625\n",
      "Epoch 86:\n",
      "worker 29/29\n",
      "    Training Loss: 0.12606458663940429\n",
      "    Training Accuracy: 0.9625\n",
      "Epoch 87:\n",
      "worker 29/29\n",
      "    Training Loss: 0.1266979138056437\n",
      "    Training Accuracy: 0.9552083333333333\n",
      "Epoch 88:\n",
      "worker 29/29\n",
      "    Training Loss: 0.18331979115804037\n",
      "    Training Accuracy: 0.9510416666666667\n",
      "Epoch 89:\n",
      "worker 29/29\n",
      "    Training Loss: 0.15871354738871257\n",
      "    Training Accuracy: 0.9552083333333333\n",
      "Epoch 90:\n",
      "worker 29/29\n",
      "    Training Loss: 0.1401562531789144\n",
      "    Training Accuracy: 0.9583333333333334\n",
      "Epoch 91:\n",
      "worker 29/29\n",
      "    Training Loss: 0.1258427063624064\n",
      "    Training Accuracy: 0.9625\n",
      "Epoch 92:\n",
      "worker 29/29\n",
      "    Training Loss: 0.14963959058125814\n",
      "    Training Accuracy: 0.946875\n",
      "Epoch 93:\n",
      "worker 29/29\n",
      "    Training Loss: 0.17820936838785809\n",
      "    Training Accuracy: 0.9489583333333333\n",
      "Epoch 94:\n",
      "worker 29/29\n",
      "    Training Loss: 0.15967187881469727\n",
      "    Training Accuracy: 0.9604166666666667\n",
      "Epoch 95:\n",
      "worker 29/29\n",
      "    Training Loss: 0.11727499961853027\n",
      "    Training Accuracy: 0.959375\n",
      "Epoch 96:\n",
      "worker 29/29\n",
      "    Training Loss: 0.15177499453226725\n",
      "    Training Accuracy: 0.9489583333333333\n",
      "Epoch 97:\n",
      "worker 29/29\n",
      "    Training Loss: 0.14408542315165201\n",
      "    Training Accuracy: 0.946875\n",
      "Epoch 98:\n",
      "worker 29/29\n",
      "    Training Loss: 0.12498541673024495\n",
      "    Training Accuracy: 0.9635416666666666\n",
      "Epoch 99:\n",
      "worker 29/29\n",
      "    Training Loss: 0.130761456489563\n",
      "    Training Accuracy: 0.95625\n"
     ]
    }
   ],
   "source": [
    "n_epochs   = 100\n",
    "lr         = 2e-2\n",
    "batch_size = 32\n",
    "\n",
    "local_models = [model.copy().send(worker) for worker in workers]\n",
    "optimizer    = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion    = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.grad = torch.zeros_like(param.data)\n",
    "\n",
    "for i_epoch in range(n_epochs):\n",
    "    print(\"Epoch {:d}:\".format(i_epoch))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    grad_shares_lists = [[] for _ in model.parameters()]\n",
    "\n",
    "    instance_count_shares = []\n",
    "\n",
    "    loss_shares           = []\n",
    "    correct_count_shares  = []\n",
    "\n",
    "    for i, (worker, local_model) in enumerate(zip(workers, local_models)):\n",
    "        print(\"worker {:d}/{:d}\".format(i, len(workers)-1), end='\\r')\n",
    "\n",
    "        dataset = federated_mnist_trainset.datasets[worker.id]\n",
    "        dataloader = data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        imgs, labels = next(iter(dataloader))\n",
    "\n",
    "        preds = local_model(imgs)\n",
    "\n",
    "        loss = criterion(preds, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        for grad_shares_list, param in zip(grad_shares_lists, local_model.parameters()):\n",
    "            grad_shares_list.append(param.grad.clone().fix_prec().share(*workers, crypto_provider=worker).get())\n",
    "            param.grad.zero_()\n",
    "\n",
    "        instance_count_shares.append(torch.tensor(0, dtype=torch.long).send(worker).add_(imgs.shape[0]).share(*workers, crypto_provider=worker).get())\n",
    "        loss_shares.append(loss.data.clone().fix_prec().share(*workers, crypto_provider=worker).get())\n",
    "        correct_count_shares.append((preds.data.argmax(dim=1) == labels).sum().share(*workers, crypto_provider=worker).get())\n",
    "\n",
    "    instance_count = sum(instance_count_shares).get().item()\n",
    "\n",
    "    for param, grad_shares_list in zip(model.parameters(), grad_shares_lists):\n",
    "        param.grad.copy_(sum(grad_shares_list).get().float_prec() / instance_count)\n",
    "\n",
    "    avg_loss     = sum(loss_shares).get().float_prec().item() / instance_count\n",
    "    avg_accuracy = sum(correct_count_shares).get().item() / instance_count\n",
    "    \n",
    "    print()\n",
    "    print(\"    Training Loss:\", avg_loss)\n",
    "    print(\"    Training Accuracy:\", avg_accuracy)\n",
    "\n",
    "    optimizer.step()\n",
    "    \n",
    "    for global_param, local_params in zip(model.parameters(), zip(*[local_model.parameters() for local_model in local_models])):\n",
    "        for worker, local_param in zip(workers, local_params):\n",
    "            local_param.data.copy_(global_param.data.clone().send(worker))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T14:51:06.216694Z",
     "start_time": "2019-07-07T14:50:59.434148Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10/10\n",
      "Test Loss: 0.12317433090209962\n",
      "Test Accuracy: 0.9614\n"
     ]
    }
   ],
   "source": [
    "test_dataloader = data.DataLoader(mnist_testset, batch_size=1024)\n",
    "\n",
    "test_loss      = 0\n",
    "instance_count = 0\n",
    "correct_count  = 0\n",
    "with torch.no_grad():\n",
    "    for i, (imgs, labels) in enumerate(test_dataloader, 1):\n",
    "        print(\"Batch {:d}/{:d}\".format(i, len(test_dataloader)), end='\\r')\n",
    "        instance_count += imgs.size(0)\n",
    "\n",
    "        preds = model(imgs)\n",
    "\n",
    "        test_loss += criterion(preds, labels).item()\n",
    "        correct_count += (preds.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "print()\n",
    "print(\"Test Loss:\", test_loss / instance_count)\n",
    "print(\"Test Accuracy:\", correct_count / instance_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
